---
title: "Practical Machine Learning Course Project"
author: "Serena Chung"
date: "March 22, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##  Practical Machine Learning Prediction Assignment

This is an R Markdown document for Coursera's Practical Machine Learning Prediction Assignment.

### Background

Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. The goal of the project is to quantify how well six participants performed the barbell lifts based on the data collected from accelerometers on the belt, forearm, arm, and dumbell of these participants. More information about the dataset is available from this link: http://groupware.les.inf.puc-rio.br/har and the following publication:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

### Data

The training data for this project are available here:

&nbsp;&nbsp; &nbsp;&nbsp; https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

&nbsp;&nbsp;&nbsp;&nbsp; https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data come this website: http://groupware.les.inf.puc-rio.br/har.

```{r environment, message=FALSE}
## preliminary requirement
library(ggplot2); library(lattice); library(caret); library(corrplot);
library(randomForest); library(gbm);
set.seed(2345)
```

```{r data}
## Load data, setting #DIV/0!, NA, and empty entries as NA
pml_training <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
                         , na.strings=c("NA","#DIV/0!","") )
pml_testing  <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
                         , na.strings=c("NA","#DIV/0!","") )
```

Separate the training dataest into two sets so some can be used later for cross validation:
```{r settraining}
inTrain <- createDataPartition(pml_training$classe,p=0.7,list=FALSE)
myTraining <- pml_training[inTrain,] 
myTesting <- pml_training[-inTrain,] 
```

### Data Exploration Feature Selection

The following remove variables that have no physical meaning with respect to the prediction:  user_name is removed because the assumption is that the prediction should not dependent on knowing who is performing the activities; X is an index and thus not useful for prediction; the time releated variables and num_window are also not useful.  This will reduece the number of variables from 160 to 154.
```{r trim}
myTraining <- subset(myTraining,select=-c(X,user_name,raw_timestamp_part_1
                                          ,raw_timestamp_part_2,cvtd_timestamp,num_window
                                          )
                     )
```

To focus on features with good amount of data, remove features with more than 80% of the values being NA:
```{r fNA}
fNA <- sapply(myTraining, function(x) sum(is.na(x)) /length(x))
myTraining <- myTraining[,fNA<=0.2]
```

Following Velloso et al. (2013), use a correlation based feature selector to select features.
This results in only seven features being selected for building a predictor model.
```{r features}
library(FSelector)
selected <- cfs(classe~.,myTraining)
myTraining<-subset(myTraining,select=c(selected,"classe")) 
names(myTraining)
```

Below is a plot showing the correlations between all remaining features. The correlation coefficients are all less than 0.9, though some are between 0.7 and 0.83. Keep all of the seven remaining features to be the conservative side.
```{r corrplot}
library(corrplot)
corrMatrix <- cor(myTraining[,-ncol(myTraining)],use="complete")
corrplot.mixed(corrMatrix, lower="color", upper="color"
               ,tl.pos="lt", tl.cex=0.8,tl.srt=90,diag="n", order="hclust"
               ,hclust.method="complete"
               )
```

### Building the Model

Build two predictor models, one using random forest and boosted tree approaches. Use 10-fold cross validation for both approaches.  The accuracies of the two models are compared using the test data set that was set aside earlier. The random forest approach gives more accurate results at 98% vs. 89% accuracy and is chosen as the final model.  
```{r models, message=FALSE, results="hide"}
modFitRF <- train(classe~.,data=myTraining,method="rf"
                          ,na.action=na.omit
                          ,trControl=trainControl(method="cv",number=10)
                          )
modFitGBM <- train(classe~.,data=myTraining,method="gbm"
                          ,na.action=na.omit
                          ,trControl=trainControl(method="cv",number=10)
                          )
mRF <- confusionMatrix(myTesting$classe,predict(modFitRF,myTesting))
mGBM <- confusionMatrix(myTesting$classe,predict(modFitGBM,myTesting))
```

```{r print}
print(mRF)
```

The overall expected out of sample error is 98.2% as indicated by the above confusion matrix output when tested against the test set.  Below is a plot of the normalized confusion matrix.  The accuracy is 97% or greater for all classifications. 
```{r confusion}
tRF <- mRF$table
tRFnormalized <- tRF/rowSums(tRF)
dfRFnormalized <- as.data.frame(tRFnormalized)
p <- ggplot(dfRFnormalized, aes(x = Prediction, y = Reference, fill = Freq)) +
    geom_tile(colour="black")+
    scale_fill_gradientn(breaks=seq(from=0, to=1, by=.1),colours=c("white","yellow","red")) +
    geom_text(aes(fill = dfRFnormalized$Freq, label = sprintf("%5.3f",dfRFnormalized$Freq)))+
    scale_x_discrete(name="Predicted Class") +
    scale_y_discrete(name="Actual Class") +
    ggtitle("Normalized Confusion Matrix")
show(p)
```

